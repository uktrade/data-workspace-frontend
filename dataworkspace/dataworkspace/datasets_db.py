from collections import defaultdict, deque
import hashlib
import json
import logging
from typing import Tuple

import pglast
import psycopg2
from psycopg2 import sql
from psycopg2.sql import Literal, SQL
import pytz
from django.conf import settings
from django.db import connections

from dataworkspace.apps.datasets.constants import DataSetType
from dataworkspace.utils import TYPE_CODES_REVERSED

logger = logging.getLogger("app")


def get_columns(database_name, schema=None, table=None, query=None, include_types=False):
    if table is not None and schema is not None:
        source = psycopg2.sql.SQL("{}.{}").format(
            psycopg2.sql.Identifier(schema), psycopg2.sql.Identifier(table)
        )
    elif query is not None:
        source = psycopg2.sql.SQL("({}) AS custom_query".format(query.rstrip(";")))
    else:
        raise ValueError("Either table or query are required")

    with connections[database_name].cursor() as cursor:
        try:
            cursor.execute(psycopg2.sql.SQL("SELECT * from {} WHERE false").format(source))

            if include_types:
                return [
                    (c[0], TYPE_CODES_REVERSED.get(c[1], "Unknown")) for c in cursor.description
                ]

            return [c[0] for c in cursor.description]
        except Exception:  # pylint: disable=broad-except
            logger.error("Failed to get dataset fields", exc_info=True)
            return []


def get_earliest_tables_last_updated_date(database_name: str, tables: Tuple[Tuple[str, str]]):
    """
    Return the earliest of the last updated dates for a list of tables in UTC.
    """
    with connections[database_name].cursor() as cursor:
        cursor.execute(
            """
            SELECT MIN(modified_date),MIN(swap_table_date)
            FROM (
                SELECT
                    table_schema,
                    table_name,
                    MAX(source_data_modified_utc) AS modified_date,
                    MAX(dataflow_swapped_tables_utc) AS swap_table_date
                FROM dataflow.metadata
                WHERE (table_schema, table_name) IN %s
                AND data_type != %s
                GROUP BY (1, 2)
            ) a
            """,
            [tables, DataSetType.REFERENCE],
        )
        modified_date, swap_table_date = cursor.fetchone()
        dt = modified_date or swap_table_date
        return dt.replace(tzinfo=pytz.UTC) if dt else None


def get_all_tables_last_updated_date(tables: Tuple[Tuple[str, str, str]]):
    """
    Return the last updated dates in UTC for each table in a list of tables
    """

    def last_updated_date_for_database(database_name, tables_for_database: Tuple[Tuple[str, str]]):
        with connections[database_name].cursor() as cursor:
            cursor.execute(
                """
                SELECT
                    table_schema,
                    table_name,
                    MAX(source_data_modified_utc) AS modified_date,
                    MAX(dataflow_swapped_tables_utc) AS swap_table_date
                FROM dataflow.metadata
                WHERE (table_schema, table_name) IN %s
                GROUP BY (1, 2)
                """,
                [tuple(tables_for_database)],
            )
            return dict(
                (
                    (((table_schema, table_name), modified_date or swap_table_date))
                    for table_schema, table_name, modified_date, swap_table_date in cursor.fetchall()
                )
            )

    tables_by_database = defaultdict(list)
    for database_name, table_schema, table_name in tables:
        tables_by_database[database_name].append((table_schema, table_name))

    return {
        database_name: last_updated_date_for_database(database_name, tables_for_database)
        for database_name, tables_for_database in tables_by_database.items()
    }


def extract_queried_tables_from_sql_query(query, log_errors=True):
    """
    Returns a list of (schema, table) tuples extracted from the passed PostgreSQL query

    This does not communicate with a database, and instead uses pglast to parse the
    query. However, it does not use pglast's built-in functions to extract tables -
    they're buggy in the cases where CTEs have the same names as tables in the
    search path.

    This isn't perfect though - it assumes tables without a schema are in the "public"
    schema, but "public" might not be in the search path, or it might not be the only
    schema in the search path. However, it's probably fine for our usage where "public"
    _is_ in the search path, and the only tables without a schema that we care about in
    our queries are indeed in the public schema - typically only reference dataset tables.
    """

    try:
        statements = pglast.parse_sql(query)
    except pglast.parser.ParseError as e:  # pylint: disable=c-extension-no-member
        if log_errors:
            logger.error(e)
        return []

    if len(statements) == 0:
        return []

    tables = set()

    node_ctenames = deque()
    node_ctenames.append((statements[0](), ()))

    while node_ctenames:
        node, ctenames = node_ctenames.popleft()

        if node.get("withClause", None) is not None:
            if node["withClause"]["recursive"]:
                ctenames += tuple((cte["ctename"] for cte in node["withClause"]["ctes"]))
                for cte in node["withClause"]["ctes"]:
                    node_ctenames.append((cte, ctenames))
            else:
                for cte in node["withClause"]["ctes"]:
                    node_ctenames.append((cte, ctenames))
                    ctenames += (cte["ctename"],)

        if node.get("@", None) == "RangeVar" and (
            node["schemaname"] is not None or node["relname"] not in ctenames
        ):
            tables.add((node["schemaname"] or "public", node["relname"]))

        for node_type, node_value in node.items():
            if node_type == "withClause":
                continue
            for nested_node in node_value if isinstance(node_value, tuple) else (node_value,):
                if isinstance(nested_node, dict):
                    node_ctenames.append((nested_node, ctenames))

    return sorted(list(tables))


def get_source_table_changelog(source_table):
    with connections[source_table.database.memorable_name].cursor() as cursor:
        cursor.execute(
            SQL(
                """
                SELECT id, source_data_modified_utc, table_schema||'.'||table_name, table_structure, data_hash_v1
                FROM dataflow.metadata
                WHERE data_type = {}
                AND table_schema = {}
                AND table_name = {}
                AND source_data_modified_utc IS NOT NULL
                AND table_structure IS NOT NULL
                ORDER BY id ASC;
                """
            ).format(
                Literal(DataSetType.MASTER),
                Literal(source_table.schema),
                Literal(source_table.table),
            )
        )
        return get_changelog_from_metadata_rows(cursor.fetchall())


def get_custom_dataset_query_changelog(query):
    with connections[query.database.memorable_name].cursor() as cursor:
        cursor.execute(
            SQL(
                """
                SELECT id, source_data_modified_utc, table_schema||'.'||table_name, table_structure, data_hash_v1
                FROM dataflow.metadata
                WHERE data_type = {}
                AND {} = any(data_ids)
                AND source_data_modified_utc IS NOT NULL
                AND table_structure IS NOT NULL
                ORDER BY id ASC
                LIMIT 10;
                """
            ).format(
                Literal(DataSetType.DATACUT),
                Literal(str(query.id)),
            )
        )
        return get_changelog_from_metadata_rows(cursor.fetchall())


def get_reference_dataset_changelog(dataset):
    db_name = list(settings.DATABASES_DATA.items())[0][0]
    with connections[db_name].cursor() as cursor:
        cursor.execute(
            SQL(
                """
                SELECT id, source_data_modified_utc, table_schema||'.'||table_name, table_structure, data_hash_v1
                FROM dataflow.metadata
                WHERE data_type = {}
                AND table_schema = 'public'
                AND table_name = {}
                AND source_data_modified_utc IS NOT NULL
                AND table_structure IS NOT NULL
                ORDER BY id ASC;
                """
            ).format(
                Literal(DataSetType.REFERENCE),
                Literal(dataset.table_name),
            )
        )
        return get_changelog_from_metadata_rows(cursor.fetchall())


def get_data_hash(cursor, sql):
    statements = pglast.parse_sql(sql)
    if statements[0].stmt()["sortClause"]:
        hashed_data = hashlib.md5()
        cursor.execute(SQL(f"SELECT t.*::TEXT FROM ({sql}) as t"))
        for row in cursor:
            hashed_data.update(row[0].encode("utf-8"))
        return hashed_data.digest()
    return None


def get_rows_number(cursor, sql):
    cursor.execute(SQL(f"SELECT count(*) FROM ({sql}) as t"))
    return cursor.fetchone()[0]


def get_changelog_from_metadata_rows(rows):
    if not rows:
        return []

    # Always add the first row to the change log
    changelog = [
        {
            "change_id": rows[0][0],
            "change_date": rows[0][1].replace(tzinfo=pytz.UTC),
            "table_name": rows[0][2],
            "previous_table_name": None,
            "table_structure": json.loads(rows[0][3]) if rows[0][3] else None,
            "previous_table_structure": None,
            "data_hash": rows[0][4],
            "previous_data_hash": None,
        }
    ]

    # Â zip(rows, rows[1:]): [1,2,3,4,5] --> [(1,2), (2,3), (3,4), (4,5)]
    for row, next_row in zip(rows, rows[1:]):
        _, _, row_table_name, row_table_structure, row_data_hash = row
        (
            next_row_id,
            next_row_change_date,
            next_row_table_name,
            next_row_table_structure,
            next_row_data_hash,
        ) = next_row
        if (
            row_table_structure != next_row_table_structure
            or row_data_hash != next_row_data_hash
            or row_table_name != next_row_table_name
        ):
            changelog.append(
                {
                    "change_id": next_row_id,
                    "change_date": next_row_change_date.replace(tzinfo=pytz.UTC),
                    "table_name": next_row_table_name,
                    "previous_table_name": row_table_name,
                    "table_structure": (
                        json.loads(next_row_table_structure) if next_row_table_structure else None
                    ),
                    "previous_table_structure": (
                        json.loads(row_table_structure) if row_table_structure else None
                    ),
                    "data_hash": next_row_data_hash,
                    "previous_data_hash": row_data_hash,
                }
            )

    return list(reversed(changelog))


def get_pipeline_last_success_date(pipeline_name):
    with connections[list(settings.DATABASES_DATA.items())[0][0]].cursor() as cursor:
        try:
            cursor.execute(
                SQL(
                    """
                    WITH dag_runs AS (
                        SELECT *
                        FROM dataflow.pipeline_dag_runs_v2
                        WHERE pipeline_name = {}
                        AND pipeline_active = 'active'
                    )
                    SELECT last_success_of_day
                    FROM dag_runs
                    WHERE dag_runs.run_end_date = (
                        SELECT MAX(run_end_date)
                        FROM dag_runs
                    );
                    """
                ).format(
                    Literal(DataSetType.MASTER),
                    Literal(pipeline_name),
                )
            )
        except Exception:  # pylint: disable=broad-except
            logger.error(
                "Failed to get last success date for pipeline %s", pipeline_name, exc_info=True
            )
            return None
        result = cursor.fetchone()
        return result[0] if result else None


def get_latest_row_count_for_table(table):
    with connections[table.database.memorable_name].cursor() as cursor:
        try:
            cursor.execute(
                SQL(
                    """
                    WITH metadata AS (
                        SELECT *
                        FROM dataflow.metadata
                        WHERE table_schema={}
                        AND table_name={}
                    )
                    SELECT number_of_rows
                    FROM metadata
                    WHERE metadata.source_data_modified_utc = (
                        SELECT MAX(source_data_modified_utc)
                        FROM metadata
                    );
                    """
                ).format(
                    Literal(table.schema),
                    Literal(table.table),
                )
            )
        except Exception:  # pylint: disable=broad-except
            logger.error("Failed to get row count for table", exc_info=True)
            return None
        result = cursor.fetchone()
        return result[0] if result else None


def get_latest_row_count_for_query(query):
    with connections[query.database.memorable_name].cursor() as cursor:
        try:
            cursor.execute(
                SQL(
                    """
                    WITH metadata AS (
                        SELECT *
                        FROM dataflow.metadata
                        WHERE data_type = {}
                        AND {} = any(data_ids)
                    )
                    SELECT number_of_rows
                    FROM metadata
                    WHERE metadata.source_data_modified_utc = (
                        SELECT MAX(source_data_modified_utc)
                        FROM metadata
                    );
                    """
                ).format(
                    Literal(DataSetType.DATACUT),
                    Literal(str(query.id)),
                )
            )
        except Exception:  # pylint: disable=broad-except
            logger.error("Failed to get row count for query", exc_info=True)
            return None
        result = cursor.fetchone()
        return result[0] if result else None


def get_all_source_tables():
    """
    Returns a list of all source tables in the datasets db.
    """
    db_conf = list(settings.DATABASES_DATA.items())[0][1]
    with psycopg2.connect(
        f"""
        host={db_conf["HOST"]} port={db_conf["PORT"]}
        dbname={db_conf["NAME"]} user={db_conf["USER"]}
        password={db_conf["PASSWORD"]} sslmode=require
        """,
        options="-c idle_in_transaction_session_timeout=5s -c statement_timeout=5s",
    ) as conn:
        with conn.cursor() as cur:
            cur.execute(
                sql.SQL(
                    """
                    SELECT table_schema || '.' || table_name FROM information_schema.tables
                    """
                )
            )
            return [x[0] for x in cur.fetchall()]


def get_pipeline_id_for_source_table(source_table):
    with connections[source_table.database.memorable_name].cursor() as cursor:
        try:
            cursor.execute(
                SQL(
                    """
                  WITH metadata AS (
                        SELECT *
                        FROM dataflow.metadata
                        where table_schema = {}
                        and table_name = {}
                        and pipeline_name is not null
                    )
                    SELECT pipeline_name
                    FROM metadata
                    WHERE metadata.source_data_modified_utc = (
                        SELECT MAX(source_data_modified_utc)
                        FROM metadata
                    );
                    """
                ).format(
                    Literal(source_table.schema),
                    Literal(source_table.table),
                )
            )
        except Exception:  # pylint: disable=broad-except
            logger.error("Failed to get pipeline id for table", exc_info=True)
            return None
        result = cursor.fetchone()
        return result[0] if result else None


def get_last_run_state_for_pipeline(pipeline_name):
    with connections[list(settings.DATABASES_DATA.items())[0][0]].cursor() as cursor:
        try:
            cursor.execute(
                SQL(
                    """
                    WITH dag_runs AS (
                        SELECT *
                        FROM dataflow.pipeline_dag_runs_v2
                        WHERE pipeline_name = {}
                        AND pipeline_active = 'active'
                    )
                    SELECT final_state
                    FROM dag_runs
                    WHERE dag_runs.run_end_date = (
                        SELECT MAX(run_end_date)
                        FROM dag_runs
                    );
                    """
                ).format(
                    Literal(pipeline_name),
                )
            )
        except Exception:  # pylint: disable=broad-except
            logger.error(
                "Failed to get last run state for pipeline %s", pipeline_name, exc_info=True
            )
            return None
        result = cursor.fetchone()
        return result[0] if result else None
